{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9134042d",
   "metadata": {},
   "source": [
    "# Assertion Generation for Existing Questions\n",
    "\n",
    "This notebook demonstrates how to generate assertions for existing data-local and data-global questions that were previously generated without assertions (e.g., when `max_assertions=0` was used or assertions were disabled during question generation).\n",
    "\n",
    "This is useful when you want to retroactively add assertion-based evaluation capabilities to existing question sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b919966",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright (c) 2025 Microsoft Corporation.\n",
    "\n",
    "import sys\n",
    "\n",
    "sys.path.insert(1, \"../../../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "991bef76",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext dotenv\n",
    "%dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d056812",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "\n",
    "from pydantic import SecretStr\n",
    "\n",
    "from benchmark_qed.autoq.io.question import (\n",
    "    load_questions,\n",
    "    save_questions,\n",
    ")\n",
    "from benchmark_qed.autoq.question_gen.data_questions.assertion_gen import (\n",
    "    AssertionValidator,\n",
    ")\n",
    "from benchmark_qed.config.llm_config import LLMConfig, LLMProvider\n",
    "from benchmark_qed.llm.factory import ModelFactory\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "if logging.getLogger(\"httpx\") is not None:\n",
    "    logging.getLogger(\"httpx\").setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "626735ae",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bc61323",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA CONFIGS\n",
    "OUTPUT_QUESTIONS_PATH = \"../../output/AP_news/questions\"\n",
    "\n",
    "# MODEL CONFIGS\n",
    "API_KEY = SecretStr(os.getenv(\"OPENAI_API_KEY\", \"\"))\n",
    "LLM_MODEL = \"gpt-4.1\"\n",
    "LLM_PARAMS = {\n",
    "    \"temperature\": 0.0,\n",
    "    \"seed\": 42,\n",
    "}  # adjust this based on your model. For example, some reasoning models do not support temperature settings\n",
    "\n",
    "# CONCURRENCY CONFIGS\n",
    "CONCURRENT_REQUESTS = 8  # Concurrent LLM calls. Adjust based on your model capacity.\n",
    "\n",
    "# ASSERTION GENERATION CONFIGS\n",
    "MAX_ASSERTIONS = 20  # Maximum number of assertions per question\n",
    "BATCH_SIZE = 100  # Batch size for processing claims in global assertion generation\n",
    "MAX_DATA_TOKENS = (\n",
    "    32000  # Maximum input data tokens for the reduce step in global assertions\n",
    ")\n",
    "ENABLE_VALIDATION = True  # Set to True to validate assertions against sources\n",
    "MIN_VALIDATION_SCORE = 3  # Minimum score (1-5) for grounding, relevance, verifiability\n",
    "\n",
    "# Parallelism for assertion generation (adjust based on your model rate limits)\n",
    "CONCURRENT_LOCAL_QUESTIONS = 8  # Questions to process in parallel for local assertions\n",
    "CONCURRENT_GLOBAL_QUESTIONS = 2  # Questions to process in parallel for global assertions (lower due to internal parallelism, set to 1 for sequential)\n",
    "\n",
    "llm = ModelFactory.create_chat_model(\n",
    "    model_config=LLMConfig(\n",
    "        model=LLM_MODEL,\n",
    "        api_key=API_KEY,\n",
    "        llm_provider=LLMProvider.OpenAIChat,\n",
    "        call_args=LLM_PARAMS,\n",
    "    )\n",
    ")\n",
    "\n",
    "# Create validator if validation is enabled\n",
    "# Validator checks assertions for:\n",
    "# - Grounding: Is the assertion supported by source texts?\n",
    "# - Relevance: Is it useful for evaluating the question?\n",
    "# - Verifiability: Is it clear and testable?\n",
    "validator = (\n",
    "    AssertionValidator(\n",
    "        llm=llm,\n",
    "        llm_params=LLM_PARAMS,\n",
    "        min_criterion_score=MIN_VALIDATION_SCORE,\n",
    "        concurrent_validations=CONCURRENT_REQUESTS,\n",
    "    )\n",
    "    if ENABLE_VALIDATION\n",
    "    else None\n",
    ")\n",
    "\n",
    "if validator:\n",
    "    print(f\"Validation enabled (min score: {MIN_VALIDATION_SCORE}/5)\")\n",
    "else:\n",
    "    print(\"Validation disabled\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77c3ccb2",
   "metadata": {},
   "source": [
    "## Generate Assertions for Existing Data-Local Questions\n",
    "\n",
    "Generate assertions for data-local questions that were previously created without assertions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b1f44c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from benchmark_qed.autoq.question_gen.data_questions.assertion_gen.local_claim_assertion_gen import (\n",
    "    LocalClaimAssertionGenerator,\n",
    ")\n",
    "\n",
    "# Load existing data-local questions from disk\n",
    "# Replace with your actual path to existing questions\n",
    "existing_local_questions = load_questions(\n",
    "    f\"{OUTPUT_QUESTIONS_PATH}/data_local_questions/selected_questions.json\"\n",
    ")\n",
    "\n",
    "print(f\"Loaded {len(existing_local_questions)} existing data-local questions\")\n",
    "\n",
    "# Filter questions that have claims\n",
    "questions_with_claims = [\n",
    "    q\n",
    "    for q in existing_local_questions\n",
    "    if hasattr(q, \"attributes\") and q.attributes and \"claims\" in q.attributes\n",
    "]\n",
    "questions_without_claims = [\n",
    "    q\n",
    "    for q in existing_local_questions\n",
    "    if not (hasattr(q, \"attributes\") and q.attributes and \"claims\" in q.attributes)\n",
    "]\n",
    "\n",
    "print(\n",
    "    f\"Questions with claims: {len(questions_with_claims)}, without claims: {len(questions_without_claims)}\"\n",
    ")\n",
    "\n",
    "# Initialize local assertion generator with optional validator\n",
    "local_assertion_generator = LocalClaimAssertionGenerator(\n",
    "    llm=llm,\n",
    "    max_assertions=MAX_ASSERTIONS,\n",
    "    validator=validator,  # Pass validator for quality filtering (None to skip)\n",
    "    max_concurrent_questions=CONCURRENT_LOCAL_QUESTIONS,  # Process questions in parallel\n",
    ")\n",
    "\n",
    "# Generate assertions for all questions with claims (parallel processing)\n",
    "await local_assertion_generator.agenerate_assertions_for_questions(\n",
    "    questions_with_claims\n",
    ")\n",
    "\n",
    "# Combine back with questions that had no claims\n",
    "updated_local_questions = questions_with_claims + questions_without_claims\n",
    "\n",
    "# Save updated questions with assertions\n",
    "save_questions(\n",
    "    updated_local_questions,\n",
    "    f\"{OUTPUT_QUESTIONS_PATH}/data_local_questions/\",\n",
    "    \"selected_questions_with_assertions\",\n",
    ")\n",
    "\n",
    "print(f\"Saved {len(updated_local_questions)} data-local questions with assertions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6585003a",
   "metadata": {},
   "source": [
    "## Generate Assertions for Existing Data-Global Questions\n",
    "\n",
    "Generate assertions for data-global questions that were previously created without assertions. Global assertion generation uses a map-reduce approach, first generating local assertions from referenced questions, then consolidating them into global assertions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1d04ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from benchmark_qed.autoq.question_gen.data_questions.assertion_gen.global_claim_assertion_gen import (\n",
    "    GlobalClaimAssertionGenerator,\n",
    ")\n",
    "\n",
    "# Load existing data-global questions from disk\n",
    "# Replace with your actual path to existing questions\n",
    "existing_global_questions = load_questions(\n",
    "    f\"{OUTPUT_QUESTIONS_PATH}/data_global_questions/selected_questions.json\"\n",
    ")\n",
    "\n",
    "print(f\"Loaded {len(existing_global_questions)} existing data-global questions\")\n",
    "\n",
    "# Filter questions that have claims\n",
    "questions_with_claims = [\n",
    "    q\n",
    "    for q in existing_global_questions\n",
    "    if hasattr(q, \"attributes\") and q.attributes and \"claims\" in q.attributes\n",
    "]\n",
    "questions_without_claims = [\n",
    "    q\n",
    "    for q in existing_global_questions\n",
    "    if not (hasattr(q, \"attributes\") and q.attributes and \"claims\" in q.attributes)\n",
    "]\n",
    "\n",
    "print(\n",
    "    f\"Questions with claims: {len(questions_with_claims)}, without claims: {len(questions_without_claims)}\"\n",
    ")\n",
    "\n",
    "# Initialize global assertion generator with optional validator\n",
    "global_assertion_generator = GlobalClaimAssertionGenerator(\n",
    "    llm=llm,\n",
    "    max_assertions=MAX_ASSERTIONS,\n",
    "    batch_size=BATCH_SIZE,  # Batch size for processing multiple claims\n",
    "    max_data_tokens=MAX_DATA_TOKENS,  # max input data tokens for the reduce step\n",
    "    concurrent_coroutines=CONCURRENT_REQUESTS,\n",
    "    validator=validator,  # Pass validator for quality filtering (None to skip)\n",
    "    max_concurrent_questions=CONCURRENT_GLOBAL_QUESTIONS,  # Process questions in parallel (lower due to internal parallelism)\n",
    ")\n",
    "\n",
    "# Generate assertions for all questions with claims (parallel processing)\n",
    "await global_assertion_generator.agenerate_assertions_for_questions(\n",
    "    questions_with_claims\n",
    ")\n",
    "\n",
    "# Combine back with questions that had no claims\n",
    "updated_global_questions = questions_with_claims + questions_without_claims\n",
    "\n",
    "# Save updated questions with assertions\n",
    "save_questions(\n",
    "    updated_global_questions,\n",
    "    f\"{OUTPUT_QUESTIONS_PATH}/data_global_questions/\",\n",
    "    \"selected_questions_with_assertions\",\n",
    ")\n",
    "\n",
    "print(f\"Saved {len(updated_global_questions)} data-global questions with assertions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f23ef6c",
   "metadata": {},
   "source": [
    "## Notes on Assertion Generation\n",
    "\n",
    "**When to use this approach:**\n",
    "- You have existing questions that were generated with `max_assertions=0` or without assertion generation\n",
    "- You want to add evaluation capabilities to previously generated question sets\n",
    "- You need to regenerate assertions with different parameters or improved prompts\n",
    "\n",
    "**Input Requirements:**\n",
    "- Questions must have `claims` in their `attributes` field\n",
    "- For data-local questions: claims should be a list of claim dictionaries\n",
    "- For data-global questions: claims can be in various formats (simple or complex)\n",
    "\n",
    "**Output Format:**\n",
    "- Assertions are added to the question's `attributes.assertions` field\n",
    "- Each assertion contains a `statement` that can be used for evaluation\n",
    "- Questions without valid claims are left unchanged\n",
    "\n",
    "**Configuration Options:**\n",
    "- `MAX_ASSERTIONS`: Maximum number of assertions to generate per question (default: 20)\n",
    "- `ENABLE_VALIDATION`: Set to `True` to validate assertions for quality (default: True)\n",
    "- `MIN_VALIDATION_SCORE`: Minimum score (1-5) for validation criteria (default: 3)\n",
    "- `BATCH_SIZE`: For global questions, controls how many claims are processed together \n",
    "- `MAX_DATA_TOKENS`: For global questions, controls the max input data tokens in the reduce step\n",
    "- `CONCURRENT_REQUESTS`: Controls parallel LLM calls for batch processing and validation\n",
    "\n",
    "**Parallelism Settings:**\n",
    "- `CONCURRENT_LOCAL_QUESTIONS`: Questions to process in parallel for local assertions (default: 8)\n",
    "- `CONCURRENT_GLOBAL_QUESTIONS`: Questions to process in parallel for global assertions (default: 2, lower due to internal parallelism; set to `1` for sequential)\n",
    "\n",
    "**Validation:**\n",
    "When `ENABLE_VALIDATION=True`, each assertion is checked for:\n",
    "- **Grounding**: Is the assertion factually supported by source texts?\n",
    "- **Relevance**: Is the assertion useful for evaluating answers to the question?\n",
    "- **Verifiability**: Is the assertion clear and objectively checkable?\n",
    "\n",
    "Assertions must score at least `MIN_VALIDATION_SCORE` on all three criteria to pass validation and be included in the final assertion set."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "benchmark-qed",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}