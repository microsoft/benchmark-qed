{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b4555ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright (c) 2025 Microsoft Corporation.\n",
    "\n",
    "import sys\n",
    "\n",
    "sys.path.insert(1, \"../../../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7365d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba3ab816",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext dotenv\n",
    "%dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fd072b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import logging\n",
    "import os\n",
    "\n",
    "import tiktoken\n",
    "\n",
    "from pydantic import SecretStr\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "from benchmark_qed.autod.data_processor.embedding import TextEmbedder\n",
    "from benchmark_qed.autod.io.text_unit import load_text_units\n",
    "\n",
    "from benchmark_qed.config.llm_config import LLMConfig, LLMProvider\n",
    "from benchmark_qed.llm.factory import ModelFactory\n",
    "\n",
    "from benchmark_qed.autoe.retrieval_scores.relevance_assessment.bing_rater import BingRelevanceRater\n",
    "from benchmark_qed.autoq.data_model.question import Question\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "if logging.getLogger(\"httpx\") is not None:\n",
    "    logging.getLogger(\"httpx\").setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b71fd92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA CONFIGS\n",
    "\n",
    "INPUT_DATA_PATH = Path(\"./example_retrieval\")\n",
    "OUTPUT_DATA_PATH = Path(\"./output/retrieval_scores\")\n",
    "\n",
    "# tokenizer used for chunking documents into text units\n",
    "ENCODING_MODEL = \"o200k_base\"\n",
    "\n",
    "# MODEL CONFIGS\n",
    "API_KEY = SecretStr(os.getenv(\"OPENAI_API_KEY\", \"\"))\n",
    "EMBEDDING_MODEL = \"text-embedding-3-large\"\n",
    "LLM_MODEL = \"gpt-4.1\"\n",
    "LLM_PARAMS = {\n",
    "    \"temperature\": 0.0,\n",
    "    \"seed\": 42,\n",
    "}  # adjust this based on your model. For example, some reasoning models do not support temperature settings\n",
    "\n",
    "EMBEDDING_LLM_CONFIG = LLMConfig(\n",
    "    model=EMBEDDING_MODEL,\n",
    "    api_key=API_KEY,\n",
    "    llm_provider=LLMProvider.OpenAIEmbedding,\n",
    ")\n",
    "\n",
    "COMPLETION_LLM_CONFIG = LLMConfig(\n",
    "        model=LLM_MODEL,\n",
    "        api_key=API_KEY,\n",
    "        llm_provider=LLMProvider.OpenAIChat,\n",
    "        call_args=LLM_PARAMS,\n",
    "    )\n",
    "\n",
    "text_embedder = TextEmbedder(\n",
    "    ModelFactory.create_embedding_model(\n",
    "        EMBEDDING_LLM_CONFIG\n",
    "    )\n",
    ")\n",
    "llm = ModelFactory.create_chat_model(\n",
    "    model_config=COMPLETION_LLM_CONFIG\n",
    "\n",
    ")\n",
    "\n",
    "token_encoder = tiktoken.get_encoding(ENCODING_MODEL)\n",
    "\n",
    "# METRICS CONFIGS\n",
    "RAG_METHODS = [\"lazygraphrag\", \"vector_rag\"]\n",
    "QUESTION_SETS = [\"activity_local\", \"activity_global\"]\n",
    "NUM_CLUSTERS = 10 # set to None to enable auto tuning, which will be slow\n",
    "NUM_QUESTIONS = 10 # set to None to compute for all questions\n",
    "SEMANTIC_REPRESENTATIVES = 15 # Increase this to reduce classification error when generating reference context for comparison\n",
    "CENTROID_REPRESENTATIVES = 5 # Increase this to reduce classification error when generating reference context for comparison\n",
    "RELEVANCE_THRESHOLD = 2\n",
    "\n",
    "relevance_rater = BingRelevanceRater(\n",
    "    llm_client=llm,\n",
    "    llm_config=COMPLETION_LLM_CONFIG,\n",
    "    concurrent_requests=32  # Lower concurrency for cluster generation\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4ddfa9e",
   "metadata": {},
   "source": [
    "## Generate Reference Context \n",
    "\n",
    "For each query, retrieve relevant clusters and top relevant chunks per cluster, to be used to evaluate RAG's retrieval performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "008fff71",
   "metadata": {},
   "outputs": [],
   "source": [
    "from benchmark_qed.autoe.retrieval_scores.reference_gen.cluster_relevance import (\n",
    "    ClusterRelevanceRater,\n",
    ")\n",
    "from benchmark_qed.autoe.retrieval_scores.reference_gen.cluster_relevance import save_cluster_references_to_json\n",
    "from benchmark_qed.autoq.data_model.question import Question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e187886",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load text units from parquet file\n",
    "text_df = pd.read_parquet(INPUT_DATA_PATH / \"text_units.parquet\")\n",
    "if 'short_id' not in text_df.columns:\n",
    "    text_df['short_id'] = text_df.index.astype(str)\n",
    "\n",
    "corpus = load_text_units(text_df)\n",
    "print(f\"Loaded {len(corpus)} text units\")\n",
    "\n",
    "# embed text units if needed\n",
    "# skip this if you already have embeddings in your corpus\n",
    "print(f\"Embedding {len(corpus)} text units\")\n",
    "corpus = await text_embedder.embed_batch(\n",
    "    text_units=corpus,\n",
    "    batch_size=32,\n",
    ")\n",
    "print(f\"Embedded {len(corpus)} text units\")\n",
    "\n",
    "# Create cluster relevance rater with text units data\n",
    "cluster_rater = ClusterRelevanceRater(\n",
    "    text_embedder=text_embedder,\n",
    "    relevance_rater=relevance_rater,\n",
    "    corpus=corpus,  # Will perform clustering once and reuse for all queries\n",
    "    semantic_neighbors=SEMANTIC_REPRESENTATIVES,\n",
    "    centroid_neighbors=CENTROID_REPRESENTATIVES,\n",
    "    num_clusters=NUM_CLUSTERS, # set to None to tune number of clusters, but might be slow\n",
    ")\n",
    "\n",
    "print(f\"Cluster relevance rater initialized with {len(cluster_rater.clusters)} clusters\")\n",
    "\n",
    "\n",
    "for question_set in QUESTION_SETS:\n",
    "    print(f\"\\nGenerating cluster references for question set: {question_set}\")\n",
    "    \n",
    "    # Load questions from vector_rag retrieval results\n",
    "    context_path = Path(INPUT_DATA_PATH / \"vector_rag\" / f\"{question_set}.json\")\n",
    "    with open(context_path, \"r\") as f:\n",
    "        retrieval_result_dicts = json.load(f)\n",
    "    \n",
    "    # Extract questions from retrieval results\n",
    "    questions = [\n",
    "        Question(id=result[\"question_id\"], text=result[\"question_text\"])\n",
    "        for result in retrieval_result_dicts\n",
    "    ]\n",
    "    if NUM_QUESTIONS is not None:\n",
    "        questions = questions[:NUM_QUESTIONS]\n",
    "\n",
    "    print(f\"Loaded {len(questions)} questions\")\n",
    "    \n",
    "    # Generate cluster references using batch assessment\n",
    "    batch_results = await cluster_rater.assess_batch(questions)\n",
    "    \n",
    "    print(f\"Generated cluster references for {len(batch_results)} questions\")\n",
    "    \n",
    "    # Save batch results to JSON using the correct function name\n",
    "    output_path = Path(OUTPUT_DATA_PATH / \"cluster_references\" / f\"{question_set}_cluster_references.json\")\n",
    "    save_cluster_references_to_json(\n",
    "        batch_results, \n",
    "        output_path,\n",
    "        include_clusters=True,\n",
    "        clusters=cluster_rater.clusters\n",
    "    )\n",
    "\n",
    "    print(f\"Saved cluster references to {output_path}\")\n",
    "\n",
    "print(\"\\nâœ“ Cluster reference generation completed for all question sets\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36810a50",
   "metadata": {},
   "source": [
    "## Retrieval Relevance Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "483e742b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from benchmark_qed.autoe.retrieval_scores.relevance_assessment.bing_rater import BingRelevanceRater\n",
    "from benchmark_qed.autoe.data_model.retrieval_result import load_retrieval_results_from_dicts\n",
    "from benchmark_qed.autoe.retrieval_scores.scoring.retrieval_relevance import assess_batch_relevance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a68be7ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "for rag_method in RAG_METHODS:\n",
    "    print(f\"Evaluating RAG method: {rag_method}\")\n",
    "\n",
    "    for question_set in QUESTION_SETS:\n",
    "        print(f\" Evaluating question set: {question_set}\")\n",
    "\n",
    "        # load context from json file\n",
    "        context_path = Path(INPUT_DATA_PATH / rag_method / f\"{question_set}.json\")\n",
    "        with open(context_path, \"r\") as f:\n",
    "            retrieval_result_dicts = json.load(f)\n",
    "\n",
    "            retrieval_results = load_retrieval_results_from_dicts(\n",
    "                data=retrieval_result_dicts, \n",
    "                context_id_key=\"source_id\",\n",
    "                context_text_key=\"text\",\n",
    "                auto_transform_context=True  # This will ensure short_id is generated\n",
    "            )\n",
    "            if NUM_QUESTIONS is not None:\n",
    "                retrieval_results = retrieval_results[:NUM_QUESTIONS]\n",
    "\n",
    "            relevance_results = await assess_batch_relevance(\n",
    "                retrieval_results=retrieval_results,\n",
    "                relevance_rater=relevance_rater\n",
    "            )\n",
    "\n",
    "            # save relevance results to json file\n",
    "            output_path = Path(OUTPUT_DATA_PATH / rag_method / f\"{question_set}_relevance.json\")\n",
    "            relevance_results.save_to_json(output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71f4910e",
   "metadata": {},
   "source": [
    "## Calculate scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6400030d",
   "metadata": {},
   "source": [
    "### Precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f27b3768",
   "metadata": {},
   "outputs": [],
   "source": [
    "from benchmark_qed.autoe.retrieval_scores.scoring.precision import get_precision_summary\n",
    "from benchmark_qed.autoe.retrieval_scores.scoring.retrieval_relevance import BatchRelevanceResult\n",
    "\n",
    "for rag_method in RAG_METHODS:\n",
    "    for question_set in QUESTION_SETS:\n",
    "        # load relevance results from json file\n",
    "        relevance_results = BatchRelevanceResult.load_from_json(Path(OUTPUT_DATA_PATH / rag_method / f\"{question_set}_relevance.json\"))\n",
    "\n",
    "        # compute precision summary\n",
    "        precision_summary = get_precision_summary(relevance_results, relevance_threshold=RELEVANCE_THRESHOLD)\n",
    "\n",
    "        # save precision summary to json file\n",
    "        output_path = Path(OUTPUT_DATA_PATH / rag_method / f\"{question_set}_precision.json\")\n",
    "        with open(output_path, \"w\") as f:\n",
    "           json.dump(precision_summary, f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "644ec2f8",
   "metadata": {},
   "source": [
    "### Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa528e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from benchmark_qed.autoe.retrieval_scores.scoring.recall import calculate_recall\n",
    "from benchmark_qed.autoe.retrieval_scores.reference_gen.cluster_relevance import load_cluster_references_from_json\n",
    "from benchmark_qed.autoe.retrieval_scores.scoring.retrieval_relevance import BatchRelevanceResult\n",
    "\n",
    "\n",
    "for rag_method in RAG_METHODS:\n",
    "    for question_set in QUESTION_SETS:\n",
    "        print(f\"Calculating recall for {rag_method} - {question_set}\")\n",
    "        \n",
    "        # Load relevance results (QueryRelevanceResult objects)\n",
    "        relevance_results = BatchRelevanceResult.load_from_json(\n",
    "            Path(OUTPUT_DATA_PATH / rag_method / f\"{question_set}_relevance.json\")\n",
    "        )\n",
    "        \n",
    "        # Load cluster references and clusters\n",
    "        cluster_references_path = Path(OUTPUT_DATA_PATH / \"cluster_references\" / f\"{question_set}_cluster_references.json\")\n",
    "        cluster_references, clusters = load_cluster_references_from_json(cluster_references_path)\n",
    "        \n",
    "        print(f\"  Loaded {len(relevance_results.results)} relevance results\")\n",
    "        print(f\"  Loaded {len(cluster_references)} cluster references\")\n",
    "        \n",
    "        # Calculate recall metrics with cluster classification error statistics\n",
    "        recall_results = calculate_recall(\n",
    "            query_relevance_results=relevance_results.results,\n",
    "            retrieval_references=cluster_references,\n",
    "            relevance_threshold=RELEVANCE_THRESHOLD,\n",
    "            clusters=clusters,\n",
    "            use_text_unit_short_id=True\n",
    "        )\n",
    "        \n",
    "        # Save recall results to JSON file\n",
    "        output_path = Path(OUTPUT_DATA_PATH / rag_method / f\"{question_set}_recall.json\")\n",
    "        output_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        with open(output_path, \"w\") as f:\n",
    "            json.dump(recall_results, f, indent=2, default=str)\n",
    "        \n",
    "        print(f\"  âœ… Saved recall results to {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1362c15",
   "metadata": {},
   "source": [
    "### Fidelity\n",
    "\n",
    "Fidelity measures how similar the distribution of relevant text units is between reference clusters and query relevance results. It uses Jensen-Shannon divergence to quantify the similarity between these distributions:\n",
    "\n",
    "- **Fidelity = 1.0 - JS_Divergence**  \n",
    "- **Higher fidelity** = More similar distributions = Better retrieval performance\n",
    "- **Lower fidelity** = Different distributions = Retrieval may be missing key clusters or focusing on wrong areas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "823ff877",
   "metadata": {},
   "outputs": [],
   "source": [
    "from benchmark_qed.autoe.retrieval_scores.scoring.fidelity import calculate_fidelity\n",
    "from benchmark_qed.autoe.retrieval_scores.reference_gen.cluster_relevance import load_cluster_references_from_json\n",
    "from benchmark_qed.autoe.retrieval_scores.scoring.retrieval_relevance import BatchRelevanceResult\n",
    "\n",
    "\n",
    "for rag_method in RAG_METHODS:\n",
    "    for question_set in QUESTION_SETS:\n",
    "        print(f\"Calculating fidelity for {rag_method} - {question_set}\")\n",
    "        \n",
    "        # Load relevance results (QueryRelevanceResult objects)\n",
    "        relevance_results = BatchRelevanceResult.load_from_json(\n",
    "            Path(OUTPUT_DATA_PATH / rag_method / f\"{question_set}_relevance.json\")\n",
    "        )\n",
    "        \n",
    "        # Load cluster references and clusters\n",
    "        cluster_references_path = Path(OUTPUT_DATA_PATH / \"cluster_references\" / f\"{question_set}_cluster_references.json\")\n",
    "        cluster_references, clusters = load_cluster_references_from_json(cluster_references_path)\n",
    "        \n",
    "        print(f\"  Loaded {len(relevance_results.results)} relevance results\")\n",
    "        print(f\"  Loaded {len(cluster_references)} cluster references\")\n",
    "       \n",
    "        # Calculate fidelity metrics using Jensen-Shannon divergence\n",
    "        fidelity_results = calculate_fidelity(\n",
    "            query_relevance_results=relevance_results.results,\n",
    "            retrieval_references=cluster_references,\n",
    "            relevance_threshold=RELEVANCE_THRESHOLD,\n",
    "            clusters=clusters,\n",
    "            use_text_unit_short_id=True\n",
    "        )\n",
    "        \n",
    "        # Save fidelity results to JSON file\n",
    "        output_path = Path(OUTPUT_DATA_PATH / rag_method / f\"{question_set}_fidelity.json\")\n",
    "        output_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        with open(output_path, \"w\") as f:\n",
    "            json.dump(fidelity_results, f, indent=2, default=str)\n",
    "        \n",
    "        print(f\"  âœ… Saved fidelity results to {output_path}\")\n",
    "        \n",
    "print(\"\\nâœ“ Fidelity calculation completed for all RAG methods and question sets\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55d1ff8d",
   "metadata": {},
   "source": [
    "## Summary Comparison\n",
    "\n",
    "Compare precision, recall, and fidelity metrics across all RAG methods and question sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3934154",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive comparison across all metrics\n",
    "import pandas as pd\n",
    "\n",
    "comparison_data = []\n",
    "\n",
    "for rag_method in RAG_METHODS:\n",
    "    for question_set in QUESTION_SETS:\n",
    "        row_data = {\n",
    "            \"RAG_Method\": rag_method,\n",
    "            \"Question_Set\": question_set\n",
    "        }\n",
    "        \n",
    "        # Load and summarize precision metrics\n",
    "        try:\n",
    "            with open(Path(OUTPUT_DATA_PATH / rag_method / f\"{question_set}_precision.json\"), \"r\") as f:\n",
    "                precision_data = json.load(f)\n",
    "                # Use both binary and graded precision from the actual precision output\n",
    "                row_data[\"Binary_Precision\"] = precision_data[\"summary\"].get(\"macro_averaged_binary_precision\", 0)\n",
    "                row_data[\"Graded_Precision\"] = precision_data[\"summary\"].get(\"macro_averaged_graded_precision\", 0)\n",
    "        except FileNotFoundError:\n",
    "            row_data[\"Binary_Precision\"] = 0\n",
    "            row_data[\"Graded_Precision\"] = 0\n",
    "        \n",
    "        # Load and summarize recall metrics\n",
    "        try:\n",
    "            with open(Path(OUTPUT_DATA_PATH / rag_method / f\"{question_set}_recall.json\"), \"r\") as f:\n",
    "                recall_data = json.load(f)\n",
    "                # Use the correct field names from the actual recall output\n",
    "                row_data[\"Recall\"] = recall_data.get(\"macro_averaged_recall\", 0)\n",
    "                row_data[\"Cluster_Classification_Error\"] = recall_data.get(\"macro_averaged_classification_error\", 0)\n",
    "        except FileNotFoundError:\n",
    "            row_data[\"Recall\"] = 0\n",
    "            row_data[\"Cluster_Classification_Error\"] = 0\n",
    "            \n",
    "        # Load and summarize fidelity metrics\n",
    "        try:\n",
    "            with open(Path(OUTPUT_DATA_PATH / rag_method / f\"{question_set}_fidelity.json\"), \"r\") as f:\n",
    "                fidelity_data = json.load(f)\n",
    "                # Use the correct field names from the actual fidelity output\n",
    "                row_data[\"Fidelity\"] = fidelity_data.get(\"macro_averaged_fidelity\", 0)\n",
    "                row_data[\"JS_Divergence\"] = fidelity_data.get(\"macro_averaged_js_divergence\", 0)\n",
    "        except FileNotFoundError:\n",
    "            row_data[\"Fidelity\"] = 0\n",
    "            row_data[\"JS_Divergence\"] = 0\n",
    "        \n",
    "        comparison_data.append(row_data)\n",
    "\n",
    "# Create comparison DataFrame\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "\n",
    "print(\"ðŸ“Š Metrics Comparison\")\n",
    "print(\"=\" * 80)\n",
    "print(comparison_df.round(4).to_string(index=False))\n",
    "\n",
    "# Save comparison to CSV for further analysis\n",
    "comparison_output_path = Path(OUTPUT_DATA_PATH / \"metrics_comparison.csv\")\n",
    "comparison_df.to_csv(comparison_output_path, index=False)\n",
    "print(f\"\\nðŸ’¾ Saved comparison to {comparison_output_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "benchmark-qed (3.13.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
