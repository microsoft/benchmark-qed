{"config": {"lang": ["en"], "separator": "[\\s\\-]+", "pipeline": ["stopWordFilter"]}, "docs": [{"location": "", "title": "Welcome to BenchmarkQED", "text": "<pre><code>flowchart LR\n    AutoQ[\"&lt;span style='font-size:1.5em; color:black'&gt;&lt;b&gt;AutoQ&lt;/b&gt;&lt;/span&gt;&lt;br&gt;LLM synthesis of&lt;br&gt;local-to-global&lt;br&gt;queries for target&lt;br&gt;datasets\"] -- creates queries &lt;br&gt;for evaluation --&gt; AutoE[\"&lt;span style='font-size:1.5em; color:black'&gt;&lt;b&gt;AutoE&lt;/b&gt;&lt;/span&gt;&lt;br&gt;LLM evaluation of&lt;br&gt;relative answer &lt;br&gt;quality on target &lt;br&gt;metrics\"]\n    AutoE ~~~ AutoD[\"&lt;span style='font-size:1.5em; color:black'&gt;&lt;b&gt;AutoD&lt;/b&gt;&lt;/span&gt;&lt;br&gt;LLM summarization&lt;br&gt;of datasets samples&lt;br&gt;to a curated target&lt;br&gt;structures\"]\n    AutoD -- curates datasets &lt;br&gt;for evaluation --&gt; AutoE\n    AutoD -- creates dataset summaries &lt;br&gt;for query synthesis --&gt; AutoQ\n    style AutoQ fill:#a8d0ed,color:black,font-weight:normal\n    style AutoE fill:#a8d0ed,color:black,font-weight:normal\n    style AutoD fill:#a8d0ed,color:black,font-weight:normal\n    linkStyle 0 stroke:#0077b6,stroke-width:2px\n    linkStyle 2 stroke:#0077b6,stroke-width:2px\n    linkStyle 3 stroke:#0077b6,stroke-width:2px</code></pre> <p>BenchmarkQED is a suite of tools designed for automated benchmarking of retrieval-augmented generation (RAG) systems. It provides components for query generation, evaluation, and dataset preparation to facilitate reproducible testing at scale.</p>"}, {"location": "#getting-started", "title": "Getting Started", "text": ""}, {"location": "#installation-instructions", "title": "Installation Instructions", "text": "<p>Install Python 3.11+</p> <p>To get started with BenchmarkQED, you have two options:</p> <ol> <li>Install from PyPI:  <pre><code>pip install benchmark-qed\n</code></pre></li> <li>Use it from source</li> </ol>"}, {"location": "#usage", "title": "Usage", "text": "<p>The sections below describe the three main components of BenchmarkQED\u2014AutoQ, AutoE, and AutoD. You will also find step-by-step examples demonstrating how to use AutoQ and AutoE, using the Install from PyPI option.</p>"}, {"location": "#autoq", "title": "AutoQ", "text": "<p>The AutoQ component generates four synthetic query classes based on the scope and source of the dataset. </p> <ul> <li> <p>Query Scope: the extent of the dataset that the question addresses</p> <ul> <li>Local queries targeting specific details of a text corpus (e.g., \"What are the public health implications of the Alaskapox virus in Alaska?\")</li> <li>Global queries targeting general aspects of a text corpus such as common themes, trends, concerns (e.g., \"Across the dataset, what are the main public health initiatives mentioned that target underserved communities?\")</li> </ul> </li> <li> <p>Query Source: the information used to generated local and global queries</p> <ul> <li>Data-driven queries based on text sampled from the overall corpus</li> <li>Activity-driven queries based on potential activities consistent with the data</li> </ul> </li> </ul> <p>AutoQ can be configured to generate any number and distribution of synthetic queries along these classes.</p>"}, {"location": "#example", "title": "Example", "text": "<p>Please follow these steps to generate synthetic queries from the AP news dataset:</p> <ol> <li> <p>Set up your project directory: <pre><code>mkdir -p ./local/autoq_test\ncd ./local/autoq_test\n</code></pre></p> </li> <li> <p>Download the AP news dataset into the <code>input</code> subfolder:     <pre><code>mkdir ./input\nbenchmark-qed data download AP_news input\n</code></pre>     Alternatively, you can manually download the files from the AP News datasets folder.</p> </li> <li> <p>Create a configuration file: <pre><code>benchmark-qed config init autoq .\n</code></pre>     This command creates two files in the <code>./autoq_test</code> directory:</p> <ul> <li><code>.env</code>: Contains environment variables for the AutoQ pipeline. Open this file and replace <code>&lt;API_KEY&gt;</code> with your own OpenAI or Azure API key.</li> <li><code>settings.yaml</code>: Contains pipeline settings, which you can modify as needed.</li> </ul> </li> <li> <p>Generate all synthetic query classes: <pre><code>benchmark-qed autoq settings.yaml output\n</code></pre></p> </li> </ol> <p>For detailed instructions on configuring and running AutoQ from the command line, see the AutoQ CLI Documentation.</p> <p>To learn more about the query synthesis process and using AutoQ programmatically, refer to the AutoQ Notebook Example.</p>"}, {"location": "#autoe", "title": "AutoE", "text": "<p>The AutoE component automates the evaluation of RAG methods using the LLM-as-a-Judge approach. AutoE evaluates RAG-generated answers over a set of queries, which can be generated from AutoQ or from other sources. For each query, AutoE presents an LLM with pairs of answers (along with the query and target metric) in a counterbalanced order, and the model judges whether the first answer wins, loses, or ties with the second. Aggregating these judgments across multiple queries and trials yields win rates for each method. By default, AutoE compares RAG answers using four quality metrics: relevance, comprehensiveness, diversity, and empowerment, while also supporting user-defined metrics.</p> <p>When reference answers (such as ground truth or \"gold standard\" responses) are available, AutoE can evaluate RAG-generated answers against these references using metrics like correctness, completeness, or other user-defined criteria on a customizable scoring scale.</p> <p>Choosing the Right LLM Judge</p> <p>Selecting an appropriate LLM judge is crucial for reliable evaluation. Less capable models can introduce biases or produce unreliable results. To validate your judge model, start with an A/A test\u2014compare a RAG method against itself. The expected outcome is a win rate of 0.5, with no statistically significant difference between the two sets of answers. Additionally, manually review the LLM\u2019s scoring and reasoning to spot any systematic errors or biases in its judgments.</p>"}, {"location": "#example-1-relative-comparison-of-rag-methods", "title": "Example 1: Relative comparison of RAG methods", "text": "<p>Please follow these steps to perform a relative comparison of RAG methods using example question-answer data generated from the AP news dataset:</p> <ol> <li> <p>Set up your project directory: <pre><code>mkdir -p ./local/pairwise_test\ncd ./local/pairwise_test\n</code></pre></p> </li> <li> <p>Download the RAG answers into the <code>input</code> subfolder:     <pre><code>mkdir ./input\nbenchmark-qed data download example_answers input\n</code></pre>     Alternatively, you can manually copy the files inside the example answers folder.</p> </li> <li> <p>Create a configuration file for pairwise comparison: <pre><code>benchmark-qed config init autoe_pairwise .\n</code></pre>     This command creates two files in the <code>./pairwise_test</code> directory:</p> <ul> <li><code>.env</code>: Contains environment variables for the pairwise comparison tests. Open this file and replace <code>&lt;API_KEY&gt;</code> with your own OpenAI or Azure API key.</li> <li><code>settings.yaml</code>: Contains pipeline settings, which you can modify as needed.</li> </ul> </li> <li> <p>Run the pairwise comparison: <pre><code>benchmark-qed autoe pairwise-scores settings.yaml output\n</code></pre></p> </li> </ol>"}, {"location": "#example-2-scoring-of-rag-answers-against-reference-answers", "title": "Example 2: Scoring of RAG answers against reference answers", "text": "<p>Please follow these steps to score RAG answers against reference answers using example data from the AP news dataset:</p> <ol> <li> <p>Set up your project directory: <pre><code>mkdir -p ./local/reference_test\ncd ./local/reference_test\n</code></pre></p> </li> <li> <p>Download the RAG answers and reference answers into the <code>input</code> subfolder:     <pre><code>mkdir ./input\nbenchmark-qed data download example_answers input\n</code></pre>     Alternatively, you can manually copy the files inside the example data folder.</p> </li> <li> <p>Create a configuration file for reference scoring: <pre><code>benchmark-qed config init autoe_reference .\n</code></pre>     This command creates two files in the <code>./reference_test</code> directory:</p> <ul> <li><code>.env</code>: Contains environment variables for the reference scoring tests. Open this file and replace <code>&lt;API_KEY&gt;</code> with your own OpenAI or Azure API key.</li> <li><code>settings.yaml</code>: Contains pipeline settings, which you can modify as needed.</li> </ul> </li> <li> <p>Run the reference scoring: <pre><code>benchmark-qed autoe reference-scores settings.yaml output\n</code></pre> For more details on configuring and running AutoE, see the AutoE CLI Documentation.</p> </li> </ol> <p>For detailed instructions on configuring and running AutoE subcommands, please refer to the AutoE CLI Documentation.</p> <p>To learn how to use AutoE programmatically, please see the AutoE Notebook Example.</p>"}, {"location": "#autod", "title": "AutoD", "text": "<p>The AutoD component provides two main data utitilies: - Data sampling: Samples datasets to meet a target specification, defined by the number of topic clusters (breadth) and the number of samples per cluster (depth) - Data summarization: Summarizes input or output datasets in a way that reflect their topic coverage, using a map-reduce process. These summaries play an important role in the AutoQ query synthesis process, but they can also be used more broadly, such as in prompts where context space is limited. </p> <p>To learn more about AutoD's data utilities, please see: AutoD Notebook Example.</p>"}, {"location": "datasets/", "title": "Datasets", "text": "<p>BenchmarkQED offers two datasets to facilitate the development and evaluation of Retrieval-Augmented Generation (RAG) systems:</p> <ul> <li>Podcast Transcripts: Contains transcripts from 70 episodes of the Behind the Tech podcast series. This is an updated version of the dataset featured in the GraphRAG paper.</li> <li>AP News: Includes 1,397 health-related news articles from the Associated Press.</li> </ul> <p>To download these datasets programmatically, use the following commands:</p> <ul> <li>Podcast Transcripts: <pre><code>benchmark-qed data download podcast OUTPUT_DIR\n</code></pre></li> <li>AP News: <pre><code>benchmark-qed data download AP_news OUTPUT_DIR\n</code></pre></li> </ul> <p>Replace <code>OUTPUT_DIR</code> with the path to the directory where you want the dataset to be saved.</p> <p>You can also find these datasets in the datasets directory.</p>"}, {"location": "developing/", "title": "Development Guide", "text": ""}, {"location": "developing/#requirements", "title": "Requirements", "text": "Name Installation Purpose Python 3.11+ Download The library is Python-based. uv Instructions uv is used for package management and virtualenv management in Python codebases"}, {"location": "developing/#installing-dependencies", "title": "Installing dependencies", "text": "<pre><code># Install Python dependencies\nuv sync\n</code></pre>"}, {"location": "developing/#generating-synthetic-queries", "title": "Generating synthetic queries", "text": "<p>Follow these steps to generate synthetic queries using AutoQ:</p> <ol> <li> <p>Set up your project directory: <pre><code>mkdir -p ./local/autoq_test\ncd ./local/autoq_test\n</code></pre></p> </li> <li> <p>Create an <code>input</code> folder and add your input data: <pre><code>mkdir ./input\n</code></pre>     Place your input files inside the <code>./input</code> directory. To get started, you can use the AP News dataset provided in the datasets folder. To download this example dataset directly into your <code>input</code> folder, run:     <pre><code>uv run benchmark-qed data download AP_news input\n</code></pre></p> </li> <li> <p>Initialize the configuration: <pre><code>uv run benchmark-qed config init autoq .\n</code></pre>     This command creates two files in the <code>./autoq_test</code> directory:</p> <ul> <li><code>.env</code>: Stores environment variables for the AutoQ pipeline. Open this file and replace <code>&lt;API_KEY&gt;</code> with your OpenAI or Azure API key.</li> <li><code>settings.yaml</code>: Contains pipeline settings. Edit this file as needed for your use case.</li> </ul> </li> <li> <p>Generate synthetic queries: <pre><code>uv run benchmark-qed autoq settings.yaml output\n</code></pre>     This will process your input data and save the generated queries in the <code>output</code> directory.</p> </li> </ol>"}, {"location": "developing/#comparing-rag-answer-pairs", "title": "Comparing RAG answer pairs", "text": "<p>Follow these steps to compare RAG answer pairs using the pairwise scoring pipeline:</p> <ol> <li> <p>Set up your project directory: <pre><code>mkdir -p ./local/pairwise_test\ncd ./local/pairwise_test\n</code></pre></p> </li> <li> <p>Create an <code>input</code> folder and add your question-answer data: <pre><code>mkdir ./input\n</code></pre>     Copy your RAG answer files into the <code>./input</code> directory. To get started, you can use the example RAG answers available in the example data folder. To download this example dataset directly into your <code>input</code> folder, run:     <pre><code>uv run benchmark-qed data download example_answers input\n</code></pre></p> </li> <li> <p>Create a configuration file for pairwise comparison: <pre><code>uv run benchmark-qed config init autoe_pairwise .\n</code></pre>     This command creates two files in the <code>./pairwise_test</code> directory:</p> <ul> <li><code>.env</code>: Contains environment variables for the pairwise comparison tests. Open this file and replace <code>&lt;API_KEY&gt;</code> with your OpenAI or Azure API key.</li> <li><code>settings.yaml</code>: Contains pipeline settings, which you can modify as needed.</li> </ul> </li> <li> <p>Run the pairwise comparison: <pre><code>uv run benchmark-qed autoe pairwise-scores settings.yaml output\n</code></pre>     The results will be saved in the <code>output</code> directory.</p> </li> </ol>"}, {"location": "developing/#scoring-rag-answers-against-reference-answers", "title": "Scoring RAG answers against reference answers", "text": "<p>Follow these steps to score RAG answers against reference answers using example data from the AP news dataset:</p> <ol> <li> <p>Set up your project directory: <pre><code>mkdir -p ./local/reference_test\ncd ./local/reference_test\n</code></pre></p> </li> <li> <p>Create an <code>input</code> folder and add your data: <pre><code>mkdir ./input\n</code></pre>     Copy your RAG answers and reference answers into the <code>input</code> directory. To get started, you can use the example RAG answers available in the example data folder. To download this example dataset directly into your <code>input</code> folder, run:     <pre><code>uv run benchmark-qed data download example_answers input\n</code></pre></p> </li> <li> <p>Create a configuration file for reference scoring: <pre><code>uv run benchmark-qed config init autoe_reference .\n</code></pre>     This creates two files in the <code>./reference_test</code> directory:</p> <ul> <li><code>.env</code>: Contains environment variables for the reference scoring pipeline. Open this file and replace <code>&lt;API_KEY&gt;</code> with your OpenAI or Azure API key.</li> <li><code>settings.yaml</code>: Contains pipeline settings, which you can modify as needed.</li> </ul> </li> <li> <p>Run the reference scoring: <pre><code>uv run benchmark-qed autoe reference-scores settings.yaml output\n</code></pre>     The results will be saved in the <code>output</code> directory.</p> </li> </ol> <p>For detailed instructions on configuring and running AutoE subcommands, please refer to the AutoE CLI Documentation.</p> <p>To learn how to use AutoE programmatically, please see the AutoE Notebook Example.</p>"}, {"location": "developing/#diving-deeper", "title": "Diving Deeper", "text": "<p>To explore the query synthesis workflow in detail, please see the AutoQ CLI Documentation for command-line usage and the AutoQ Notebook Example for a step-by-step programmatic guide.</p> <p>For a deeper understanding of AutoE evaluation pipelines, please refer to the AutoE CLI Documentation for available commands and the AutoE Notebook Example for hands-on examples.</p>"}, {"location": "cli/autoe/", "title": "AutoE", "text": ""}, {"location": "cli/autoe/#pairwise-scoring-configuration", "title": "Pairwise Scoring Configuration", "text": "<p>This document describes the configuration schema for scoring a set of conditions using a language model. It includes definitions for conditions, evaluation criteria, and model configuration. For more information about how to configure the LLM check: LLM Configuration</p> <p>To generate a template configuration file you can run:</p> <pre><code>benchmark_qed config init autoe_pairwise local/autoe_pairwise/settings.yaml\n</code></pre> <p>See more about the config init command: Config Init CLI</p>"}, {"location": "cli/autoe/#classes-and-fields", "title": "Classes and Fields", "text": ""}, {"location": "cli/autoe/#condition", "title": "<code>Condition</code>", "text": "<p>Represents a condition to be evaluated.</p> Field Type Description <code>name</code> <code>str</code> Name of the condition. <code>answer_base_path</code> <code>Path</code> Path to the JSON file containing the answers for this condition."}, {"location": "cli/autoe/#criteria", "title": "<code>Criteria</code>", "text": "<p>Defines a scoring criterion used to evaluate conditions.</p> Field Type Description <code>name</code> <code>str</code> Name of the criterion. <code>description</code> <code>str</code> Detailed explanation of what the criterion means and how to apply it."}, {"location": "cli/autoe/#pairwiseconfig", "title": "<code>PairwiseConfig</code>", "text": "<p>Top-level configuration for scoring a set of conditions.</p> Field Type Default Description <code>base</code> <code>Condition \\| None</code> <code>None</code> The base condition to compare others against. <code>others</code> <code>list[Condition]</code> <code>[]</code> List of other conditions to compare. <code>question_sets</code> <code>list[str]</code> <code>[]</code> List of question sets to use for scoring. <code>criteria</code> <code>list[Criteria]</code> <code>pairwise_scores_criteria()</code> List of criteria to use for scoring. <code>llm_config</code> <code>LLMConfig</code> <code>LLMConfig()</code> Configuration for the LLM used in scoring. <code>trials</code> <code>int</code> <code>4</code> Number of trials to run for each condition."}, {"location": "cli/autoe/#yaml-example", "title": "YAML Example", "text": "<p>Below is an example of how this configuration might be represented in a YAML file. The API key is referenced using an environment variable.</p> <p>Save the following yaml file as autoe_pairwise_settings.yaml and use with the command:</p> <pre><code>benchmark_qed autoe pairwise-scores autoe_pairwise_settings.yaml local/output_test\n</code></pre> <p>To run autoe with our generated answers. See the CLI Reference section for more options.</p> <pre><code>base:\n  name: vector_rag\n  answer_base_path: example_answers/vector_rag\nothers:\n  - name: lazygraphrag\n    answer_base_path: example_answers/lazygraphrag\n  - name: graphrag_global\n    answer_base_path: example_answers/graphrag_global\nquestion_sets:\n  - activity_global\n  - activity_local\ntrials: 4\nllm_config:\n  auth_type: api_key\n  model: gpt-4.1\n  api_key: ${OPENAI_API_KEY}\n  llm_provider: openai.chat\n  concurrent_requests: 20\n</code></pre> <p>\ud83d\udca1 Note: The api_key field uses an environment variable reference <code>${OPENAI_API_KEY}</code>. Make sure to define this variable in a .env file or your environment before running the application.</p> <pre><code># .env file\nOPENAI_API_KEY=your-secret-api-key-here\n</code></pre>"}, {"location": "cli/autoe/#reference-based-scoring-configuration", "title": "Reference-Based Scoring Configuration", "text": "<p>This document describes the configuration schema for evaluating generated answers against a reference set using a language model. It includes definitions for reference and generated conditions, scoring criteria, and model configuration. For more information about how to configure the LLM check: LLM Configuration</p> <p>To generate a template configuration file you can run:</p> <pre><code>benchmark_qed config init autoe_reference local/autoe_reference/settings.yaml\n</code></pre> <p>See more about the config init command: Config Init CLI</p>"}, {"location": "cli/autoe/#classes-and-fields_1", "title": "Classes and Fields", "text": ""}, {"location": "cli/autoe/#condition_1", "title": "<code>Condition</code>", "text": "<p>Represents a condition to be evaluated.</p> Field Type Description <code>name</code> <code>str</code> Name of the condition. <code>answer_base_path</code> <code>Path</code> Path to the JSON file containing the answers for this condition."}, {"location": "cli/autoe/#criteria_1", "title": "<code>Criteria</code>", "text": "<p>Defines a scoring criterion used to evaluate conditions.</p> Field Type Description <code>name</code> <code>str</code> Name of the criterion. <code>description</code> <code>str</code> Detailed explanation of what the criterion means and how to apply it."}, {"location": "cli/autoe/#referenceconfig", "title": "<code>ReferenceConfig</code>", "text": "<p>Top-level configuration for scoring generated answers against a reference.</p> Field Type Default Description <code>reference</code> <code>Condition</code> required The condition containing the reference answers. <code>generated</code> <code>list[Condition]</code> <code>[]</code> List of conditions with generated answers to be scored. <code>criteria</code> <code>list[Criteria]</code> <code>reference_scores_criteria()</code> List of criteria to use for scoring. <code>score_min</code> <code>int</code> <code>1</code> Minimum score for each criterion. <code>score_max</code> <code>int</code> <code>10</code> Maximum score for each criterion. <code>llm_config</code> <code>LLMConfig</code> <code>LLMConfig()</code> Configuration for the LLM used in scoring. <code>trials</code> <code>int</code> <code>4</code> Number of trials to run for each condition."}, {"location": "cli/autoe/#yaml-example_1", "title": "YAML Example", "text": "<p>Below is an example of how this configuration might be represented in a YAML file. The API key is referenced using an environment variable.</p> <p>Save the following yaml file as autoe_reference_settings.yaml and use with the command:</p> <pre><code>benchmark_qed autoe reference-scores autoe_reference_settings.yaml local/output_test\n</code></pre> <p>To run autoe with our generated answers. See the CLI Reference section for more options.</p> <p><pre><code>reference:\n  name: lazygraphrag\n  answer_base_path: example_answers/lazygraphrag/activity_global.json\n\ngenerated:\n  - name: vector_rag\n    answer_base_path: example_answers/vector_rag/activity_global.json\n\nscore_min: 1\nscore_max: 10\n\ntrials: 4\n\nllm_config:\n  model: \"gpt-4.1\"\n  auth_type: \"api_key\"\n  api_key: ${OPENAI_API_KEY}\n  concurrent_requests: 4\n  llm_provider: \"openai.chat\"\n  init_args: {}\n  call_args:\n    temperature: 0.0\n    seed: 42\n</code></pre> \ud83d\udca1 Note: The api_key field uses an environment variable reference <code>${OPENAI_API_KEY}</code>. Make sure to define this variable in a .env file or your environment before running the application.</p> <pre><code># .env file\nOPENAI_API_KEY=your-secret-api-key-here\n</code></pre>"}, {"location": "cli/autoe/#cli-reference", "title": "CLI Reference", "text": "<p>This page documents the command-line interface of the benchmark-qed autoe package.</p>"}, {"location": "cli/autoe/#autoe", "title": "autoe", "text": "<p>No description available</p>"}, {"location": "cli/autoe/#usage", "title": "Usage", "text": "<p><code>autoe [OPTIONS] COMMAND [ARGS]...</code></p>"}, {"location": "cli/autoe/#arguments", "title": "Arguments", "text": "<p>No arguments available</p>"}, {"location": "cli/autoe/#options", "title": "Options", "text": "Name Description Required Default <code>--install-completion</code> Install completion for the current shell. No - <code>--show-completion</code> Show completion for the current shell, to copy it or customize the installation. No - <code>--help</code> Show this message and exit. No - <code>pairwise-scores</code> Generate scores for the different... No - <code>reference-scores</code> Generate scores for the generated answers... No -"}, {"location": "cli/autoe/#sub-commands", "title": "Sub Commands", "text": ""}, {"location": "cli/autoe/#autoe-pairwise-scores", "title": "<code>autoe pairwise-scores</code>", "text": "<p>Generate scores for the different conditions provided in the JSON file.</p>"}, {"location": "cli/autoe/#usage_1", "title": "Usage", "text": "<p><code>autoe pairwise-scores [OPTIONS] COMPARISON_SPEC OUTPUT</code></p>"}, {"location": "cli/autoe/#arguments_1", "title": "Arguments", "text": "Name Description Required <code>COMPARISON_SPEC</code> The path to the JSON file containing the conditions. Yes <code>OUTPUT</code> The path to the output file for the scores. Yes"}, {"location": "cli/autoe/#options_1", "title": "Options", "text": "Name Description Required Default <code>--alpha FLOAT</code> The p-value threshold for the significance test.  [default: 0.05] No - <code>--exclude-criteria TEXT</code> The criteria to exclude from the scoring. No - <code>--print-model-usage / --no-print-model-usage</code> Whether to print the model usage statistics after scoring.  [default: no-print-model-usage] No - <code>--include-score-id-in-prompt / --no-include-score-id-in-prompt</code> Whether to include the score ID in the evaluation prompt for the LLM (might be useful to avoid cached scores).  [default: include-score-id-in-prompt] No - <code>--question-id-key TEXT</code> The key in the JSON file that contains the question ID. This is used to match questions across different conditions.  [default: question_id] No - <code>--help</code> Show this message and exit. No -"}, {"location": "cli/autoe/#autoe-reference-scores", "title": "<code>autoe reference-scores</code>", "text": "<p>Generate scores for the generated answers provided in the JSON file.</p>"}, {"location": "cli/autoe/#usage_2", "title": "Usage", "text": "<p><code>autoe reference-scores [OPTIONS] COMPARISON_SPEC OUTPUT</code></p>"}, {"location": "cli/autoe/#arguments_2", "title": "Arguments", "text": "Name Description Required <code>COMPARISON_SPEC</code> The path to the JSON file containing the configuration. Yes <code>OUTPUT</code> The path to the output file for the scores. Yes"}, {"location": "cli/autoe/#options_2", "title": "Options", "text": "Name Description Required Default <code>--exclude-criteria TEXT</code> The criteria to exclude from the scoring. No - <code>--print-model-usage / --no-print-model-usage</code> Whether to print the model usage statistics after scoring.  [default: no-print-model-usage] No - <code>--include-score-id-in-prompt / --no-include-score-id-in-prompt</code> Whether to include the score ID in the evaluation prompt for the LLM (might be useful to avoid cached scores).  [default: include-score-id-in-prompt] No - <code>--question-id-key TEXT</code> The key in the JSON file that contains the question ID. This is used to match questions across different conditions.  [default: question_id] No - <code>--help</code> Show this message and exit. No -"}, {"location": "cli/autoq/", "title": "AutoQ", "text": ""}, {"location": "cli/autoq/#question-generation-configuration", "title": "Question Generation Configuration", "text": "<p>This document describes the configuration schema for the question generation process, including input data, sampling, encoding, and model settings. For more information about how to configure the LLM check: LLM Configuration</p> <p>To generate a template configuration file you can run:</p> <pre><code>benchmark_qed config init autoq local/autoq/settings.yaml\n</code></pre> <p>See more about the config init command: Config Init CLI</p>"}, {"location": "cli/autoq/#classes-and-fields", "title": "Classes and Fields", "text": ""}, {"location": "cli/autoq/#inputconfig", "title": "<code>InputConfig</code>", "text": "<p>Configuration for the input data used in question generation.</p> Field Type Default Description <code>dataset_path</code> <code>Path</code> required Path to the input dataset file. <code>input_type</code> <code>InputDataType</code> <code>CSV</code> The type of the input data (e.g., CSV, JSON). <code>text_column</code> <code>str</code> <code>\"text\"</code> The column containing the text data. <code>metadata_columns</code> <code>list[str] \\| None</code> <code>None</code> Optional list of columns containing metadata. <code>file_encoding</code> <code>str</code> <code>\"utf-8\"</code> Encoding of the input file."}, {"location": "cli/autoq/#questionconfig", "title": "<code>QuestionConfig</code>", "text": "<p>Configuration for generating standard questions.</p> Field Type Default Description <code>num_questions</code> <code>int</code> <code>20</code> Number of questions to generate per class. <code>oversample_factor</code> <code>float</code> <code>2.0</code> Factor to overgenerate questions before filtering."}, {"location": "cli/autoq/#activityquestionconfig", "title": "<code>ActivityQuestionConfig</code>", "text": "<p>Extends <code>QuestionConfig</code> with additional fields for persona-based question generation.</p> Field Type Default Description <code>num_personas</code> <code>int</code> <code>5</code> Number of personas to generate questions for. <code>num_tasks_per_persona</code> <code>int</code> <code>5</code> Number of tasks per persona. <code>num_entities_per_task</code> <code>int</code> <code>10</code> Number of entities per task."}, {"location": "cli/autoq/#encodingmodelconfig", "title": "<code>EncodingModelConfig</code>", "text": "<p>Configuration for the encoding model used to chunk documents.</p> Field Type Default Description <code>model_name</code> <code>str</code> <code>\"o200k_base\"</code> Name of the encoding model. <code>chunk_size</code> <code>int</code> <code>600</code> Size of each text chunk. <code>chunk_overlap</code> <code>int</code> <code>100</code> Overlap between consecutive chunks."}, {"location": "cli/autoq/#samplingconfig", "title": "<code>SamplingConfig</code>", "text": "<p>Configuration for sampling data from clusters.</p> Field Type Default Description <code>num_clusters</code> <code>int</code> <code>50</code> Number of clusters to sample from. <code>num_samples_per_cluster</code> <code>int</code> <code>10</code> Number of samples per cluster. <code>random_seed</code> <code>int</code> <code>42</code> Seed for reproducibility."}, {"location": "cli/autoq/#questiongenerationconfig", "title": "<code>QuestionGenerationConfig</code>", "text": "<p>Top-level configuration for the entire question generation process.</p> Field Type Default Description <code>input</code> <code>InputConfig</code> required Input data configuration. <code>data_local</code> <code>QuestionConfig</code> <code>QuestionConfig()</code> Local data question generation settings. <code>data_global</code> <code>QuestionConfig</code> <code>QuestionConfig()</code> Global data question generation settings. <code>activity_local</code> <code>ActivityQuestionConfig</code> <code>ActivityQuestionConfig()</code> Local activity question generation. <code>activity_global</code> <code>ActivityQuestionConfig</code> <code>ActivityQuestionConfig()</code> Global activity question generation. <code>concurrent_requests</code> <code>int</code> <code>8</code> Number of concurrent model requests. <code>encoding</code> <code>EncodingModelConfig</code> <code>EncodingModelConfig()</code> Encoding model configuration. <code>sampling</code> <code>SamplingConfig</code> <code>SamplingConfig()</code> Sampling configuration. <code>chat_model</code> <code>LLMConfig</code> <code>LLMConfig()</code> LLM configuration for chat. <code>embedding_model</code> <code>LLMConfig</code> <code>LLMConfig()</code> LLM configuration for embeddings."}, {"location": "cli/autoq/#yaml-example", "title": "YAML Example", "text": "<p>Here is an example of how this configuration might look in a YAML file.</p> <p>Save the following yaml file as autoq_settings.yaml and use with the command:</p> <pre><code>benchmark_qed autoq autoq_settings.yaml local/output_test\n</code></pre> <p>To run autoq with our AP news dataset. See the CLI Reference section for more options.</p> <pre><code>## Input Configuration\ninput:\n  dataset_path: datasets/AP_news/raw_data/\n  input_type: json\n  text_column: body_nitf\n  metadata_columns: [headline, firstcreated]\n  file_encoding: utf-8-sig\n\n## Encoder configuration\nencoding:\n  model_name: o200k_base\n  chunk_size: 600\n  chunk_overlap: 100\n\n## Sampling Configuration\nsampling:\n  num_clusters: 20\n  num_samples_per_cluster: 10\n  random_seed: 42\n\n## LLM Configuration\nchat_model:\n  auth_type: api_key\n  model: gpt-4.1\n  api_key: ${OPENAI_API_KEY}\n  llm_provider: openai.chat\nembedding_model:\n  auth_type: api_key\n  model: text-embedding-3-large\n  api_key: ${OPENAI_API_KEY}\n  llm_provider: openai.embedding\n\n## Question Generation Configuration\ndata_local:\n  num_questions: 10\n  oversample_factor: 2.0\ndata_global:\n  num_questions: 10\n  oversample_factor: 2.0\nactivity_local:\n  num_questions: 10\n  oversample_factor: 2.0\n  num_personas: 5\n  num_tasks_per_persona: 2\n  num_entities_per_task: 5\nactivity_global:\n  num_questions: 10\n  oversample_factor: 2.0\n  num_personas: 5\n  num_tasks_per_persona: 2\n  num_entities_per_task: 5\n</code></pre> <p>\ud83d\udca1 Note: The api_key field uses an environment variable reference <code>${OPENAI_API_KEY}</code>. Make sure to define this variable in a .env file or your environment before running the application.</p> <pre><code># .env file\nOPENAI_API_KEY=your-secret-api-key-here\n</code></pre>"}, {"location": "cli/autoq/#cli-reference", "title": "CLI Reference", "text": "<p>This page documents the command-line interface of the benchmark-qed autoq package.</p>"}, {"location": "cli/autoq/#autoq", "title": "autoq", "text": "<p>Generate questions from the input data.</p>"}, {"location": "cli/autoq/#usage", "title": "Usage", "text": "<p><code>autoq [OPTIONS] CONFIGURATION_PATH OUTPUT_DATA_PATH</code></p>"}, {"location": "cli/autoq/#arguments", "title": "Arguments", "text": "Name Description Required <code>CONFIGURATION_PATH</code> The path to the file containing the configuration. Yes <code>OUTPUT_DATA_PATH</code> The path to the output folder for the results. Yes"}, {"location": "cli/autoq/#options", "title": "Options", "text": "Name Description Required Default <code>--generation-types [data_local|data_global|activity_local|activity_global]</code> The source of the question generation. No - <code>--print-model-usage / --no-print-model-usage</code> Whether to print the model usage statistics after scoring.  [default: no-print-model-usage] No - <code>--install-completion</code> Install completion for the current shell. No - <code>--show-completion</code> Show completion for the current shell, to copy it or customize the installation. No - <code>--help</code> Show this message and exit. No -"}, {"location": "cli/config_init/", "title": "Config Init", "text": ""}, {"location": "cli/config_init/#cli-reference", "title": "CLI Reference", "text": "<p>This page documents the command-line interface of the benchmark-qed configuration package.</p>"}, {"location": "cli/config_init/#config", "title": "config", "text": "<p>Generate settings file.</p>"}, {"location": "cli/config_init/#usage", "title": "Usage", "text": "<p><code>config [OPTIONS] CONFIG_TYPE:{autoq|autoe_pairwise|autoe_reference} ROOT</code></p>"}, {"location": "cli/config_init/#arguments", "title": "Arguments", "text": "Name Description Required <code>CONFIG_TYPE:{autoq|autoe_pairwise|autoe_reference}</code> The type of configuration to generate. Options are: autoq, autoe_pairwise, autoe_reference. Yes <code>ROOT</code> The path to root directory with the input folder. Yes"}, {"location": "cli/config_init/#options", "title": "Options", "text": "Name Description Required Default <code>--install-completion</code> Install completion for the current shell. No - <code>--show-completion</code> Show completion for the current shell, to copy it or customize the installation. No - <code>--help</code> Show this message and exit. No -"}, {"location": "cli/llm_config/", "title": "LLM Configuration", "text": "<p>This document outlines the configuration options for setting up and using a Large Language Model (LLM) in benchmark_qed. It includes details on supported providers, authentication methods, and runtime parameters.</p>"}, {"location": "cli/llm_config/#llmconfig", "title": "<code>LLMConfig</code>", "text": "<p>Defines the configuration for the language model used in scoring or generation tasks.</p> Field Type Default Description <code>model</code> <code>str</code> <code>\"gpt-4.1\"</code> The name of the model to use. Must be a valid model identifier. <code>auth_type</code> <code>AuthType</code> <code>\"api_key\"</code> Authentication method. Options: <code>\"api_key\"</code> or <code>\"azure_managed_identity\"</code>. <code>api_key</code> <code>SecretStr</code> <code>\"$OPENAI_API_KEY\"</code> API key for accessing the model. Should be provided via environment variable. <code>concurrent_requests</code> <code>int</code> <code>4</code> Number of concurrent requests allowed to the model. <code>llm_provider</code> <code>LLMProvider \\| str</code> <code>\"openai.chat\"</code> Specifies the provider and type of model. See <code>LLMProvider</code> enum for options. <code>init_args</code> <code>dict[str, Any]</code> <code>{}</code> Additional arguments passed during model initialization. <code>call_args</code> <code>dict[str, Any]</code> <code>{\"temperature\": 0.0, \"seed\": 42}</code> Parameters passed when invoking the model."}, {"location": "cli/llm_config/#llmprovider-enum", "title": "<code>LLMProvider</code> Enum", "text": "<p>Defines the supported LLM providers and model types.</p> Value Description <code>openai.chat</code> OpenAI's chat-based models. <code>openai.embedding</code> OpenAI's embedding models. <code>azure.openai.chat</code> Azure-hosted OpenAI chat models. <code>azure.openai.embedding</code> Azure-hosted OpenAI embedding models. <code>azure.inference.chat</code> Azure Inference Service chat models. <code>azure.inference.embedding</code> Azure Inference Service embedding models."}, {"location": "cli/llm_config/#authtype-enum", "title": "<code>AuthType</code> Enum", "text": "<p>Specifies the authentication method used to access the model.</p> Value Description <code>api_key</code> Use a static API key for authentication. <code>azure_managed_identity</code> Use Azure Managed Identity for authentication."}, {"location": "cli/llm_config/#yaml-example-for-llmconfig", "title": "YAML Example for <code>LLMConfig</code>", "text": ""}, {"location": "cli/llm_config/#openai", "title": "OpenAI", "text": "<pre><code>llm_config:\n  model: \"gpt-4.1\"\n  auth_type: \"api_key\"\n  api_key: ${OPENAI_API_KEY}\n  concurrent_requests: 4\n  llm_provider: \"openai.chat\"\n  init_args: {}\n  call_args:\n    temperature: 0.0\n    seed: 42\n</code></pre> <p>\ud83d\udca1 Note: The api_key field uses an environment variable reference <code>${OPENAI_API_KEY}</code>. Make sure to define this variable in a .env file or your environment before running the application.</p> <pre><code># .env file\nOPENAI_API_KEY=your-secret-api-key-here\n</code></pre>"}, {"location": "cli/llm_config/#azure-openai", "title": "Azure OpenAI", "text": "<pre><code>llm_config:\n  model: \"gpt-4.1\"\n  auth_type: \"azure_managed_identity\" # or api_key like the example above\n  concurrent_requests: 4\n  llm_provider: \"azure.openai.chat\"\n  init_args: \n    api_version: 2024-12-01-preview\n    azure_endpoint: https://&lt;instance&gt;.openai.azure.com # Replace &lt;instance&gt; with the actual value\n  call_args:\n    temperature: 0.0\n    seed: 42\n</code></pre> <p>\ud83d\udca1 Note: If you use azure_manager_identity make sure to be authenticated with <code>az login</code> on a terminal, if you use api_key make sure to include the api reference and the .env file.</p>"}]}